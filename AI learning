reading a csv 

encoding used to read some special chars
table = pd.read_csv('Car_Purchasing_Data.csv',encoding="ISO-8859-1")


drop following from axis 1 == Y  0 == X 
new_table  = table.drop(['Customer Name','Country','Customer e-mail','Car Purchase Amount'],axis=1)

import seaborn as sns
sns.pairplot(new_table)
plot data



scaling data 
from sklearn.preprocessing import MinMaxScaler

scaler= MinMaxScaler()
new_Scaled = scaler.fit_transform(new_table)

new_Scaled

[0.        , 0.4370344 , 0.53515116, 0.57836085, 0.22342985],
       [0.        , 0.41741247, 0.58308616, 0.476028  , 0.52140195],
       [1.        , 0.46305795, 0.42248189, 0.55579674, 0.63108896],
       ...,
       [1.        , 0.67886994, 0.61110973, 0.52822145, 0.75972584],
       [1.        , 0.78321017, 0.37264988, 0.69914746, 0.3243129 ],
       [1.        , 0.53462305, 0.51713347, 0.46690159, 0.45198622]])




new_Scaled.shape





reshaping a data column for further process 
y.values.reshape(-1,1)





from tensorflow import keras

from keras.models import Sequential
from keras.layers import Dense


model=  Sequential()

model.add(Dense(25, input_dim =5 , activation='relu'))  1st input layer

model.add(Dense(25, activation='relu'))  hidden layer 

model.add(Dense(1, activation='linear'))  output layer






epochs  and batch size tweak to reduce mask 
epochs_hist = model.fit(X_train,y_train ,epochs=100, batch_size=50, verbose=1,validation_split=0.2)
# validation is snaity check data , overfitting  , ,  verbose = info level to display,  





summary  
read csv drop any extra irrelevant data / columns 

split input and ouput data 

X =  certain set of columnes df[a,b,c]
Y=  output expected  df[o] 


scale them  
from sklearn.preprocessing import MinMaxScaler

scaler= MinMaxScaler()
X_scaled  = scaler.fit_transform(X)



y.values.reshape(-1,1) if single frame is there 
Y_Scaled =  scalar.fit_transform(Y)

now data is scaled in similar form or range 



create model input hidden and output layer giving input_dim 
as number of frames in input 

split data 
from sklearn.model_selection import train_test_split

X_train,X_test , y_train, y_test = train_test_split(new_Scaled,scaled_y)

compile the data 
model.compile(optimizer= 'adam', loss='mean_squared_error')


train the model 
epochs_hist = model.fit(X_train,y_train ,epochs=100, batch_size=50, verbose=1,validation_split=0.2)
# validation is snaity check data , overfitting  , ,  verbose = info level to display,  


plot if needed to see 
epochs_hist.history.keys()

plt.plot(epochs_hist.history['loss'])
plt.plot(epochs_hist.history['val_loss'])                             



give a input to predict 
X_test_sample = np.array([[0, 0.4370344,  0.53515116, 0.57836085, 0.22342985]])
y_predict_sample = model.predict(X_test_sample)


transformed or scaled input is needed as input is trained with scaled data




